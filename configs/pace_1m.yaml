model:
  input_dim: 4096  # LLaMA-2 hidden dimension (will be auto-detected)
  hidden_dim: 16384  # 4x overcomplete
  k: 512  # Top-k sparsity (~3% of hidden_dim)
  tied_weights: true
  normalize_decoder: true
  bias: true
  dtype: float32

data:
  concept_dir: "./concept"
  concept_index_path: "./concept_index.txt"
  max_concepts: 10000  # Use first 10k concepts for training
  normalize: true
  train_val_split: 0.8

training:
  num_epochs: 10
  learning_rate: 0.001
  optimizer: "adam"
  weight_decay: 0.0001
  l1_lambda: 0.0001  # L1 sparsity penalty
  grad_clip_norm: 1.0
  batch_size: 128
  val_batch_size: 256
  eval_every: 500
  save_every: 2000
  log_every: 100
  max_steps: null
  early_stopping_patience: null
  use_wandb: true
  wandb_project: "topk-sae-pace"
  log_dir: "logs/pace"
  checkpoint_dir: "checkpoints/pace" 