model:
  input_dim: 2304  # Gemma2-2b hidden dimension
  hidden_dim: 9216  # 4x overcomplete
  k: 128  # Top-k sparsity (~5.5% of hidden_dim)
  tied_weights: true
  normalize_decoder: true
  bias: true
  dtype: float32

data:
  model_name: "google/gemma-2-2b"
  layer_name: "model.layers.15"  # Middle layer (out of 26 layers)
  dataset_name: "openwebtext"
  dataset_split: "train"
  num_samples: 100000  # 100k samples for training
  max_length: 512
  batch_size: 8  # For activation collection
  streaming: false
  cache_dir: null
  save_activations_path: null

training:
  num_epochs: 5
  learning_rate: 0.001
  optimizer: "adam"
  weight_decay: 0.0001
  l1_lambda: 0.0001  # L1 sparsity penalty
  grad_clip_norm: 1.0
  batch_size: 256  # Training batch size
  val_batch_size: 512
  train_val_split: 0.9
  scheduler: null
  scheduler_params: {}
  eval_every: 1000
  save_every: 5000
  log_every: 100
  max_steps: null
  early_stopping_patience: null
  use_wandb: true
  wandb_project: "topk-sae-gemma2-2b"
  log_dir: "logs"
  checkpoint_dir: "checkpoints" 